{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 02 \\\\\\ Examples for lecture\n",
    "# Rock the Vote!\n",
    "\n",
    "## Did TV advertisements affect voter turnout among 18-19 year olds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reading json files\n",
    "import json\n",
    "\n",
    "# numerical libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pystan\n",
    "\n",
    "# pandas!\n",
    "import pandas as pd\n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to print a long string nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(info,wpl=12):\n",
    "    \"\"\"\n",
    "    nicely print a long paragraph\n",
    "    \"\"\"\n",
    "    \n",
    "    long_info = info.split()\n",
    "    num_lines = round(len(long_info) / wpl)\n",
    "    \n",
    "    info_break = []\n",
    "    \n",
    "    # break up the long string into multiple lines\n",
    "    for i in range(num_lines):\n",
    "        hld = ''\n",
    "        chunk = long_info[wpl*i:wpl*(i+1)]\n",
    "        \n",
    "        # piece each line into one string\n",
    "        for i in range(len(chunk)):\n",
    "            hld = hld + chunk[i] + ' '\n",
    "        \n",
    "        info_break.append(hld)\n",
    "    \n",
    "    # now print!\n",
    "    for i in range(len(info_break)):\n",
    "        print(info_break[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vars(var_dict):\n",
    "    \"\"\"\n",
    "    nicely print the infomation about each variable\n",
    "    \"\"\"\n",
    "    # what's the longest variable name?\n",
    "    max_len = 0\n",
    "    for k in var_dict.keys():\n",
    "        if len(k) > max_len:\n",
    "            max_len = len(k)\n",
    "    \n",
    "    for k in var_dict.keys():\n",
    "        len_k = len(k)\n",
    "        print(str(k) + ' '*(max_len - len_k + 1) + ' :::  ' + var_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Example 2: Rock the Vote\n",
    "\n",
    "Jackman presents this example in his _Bayesian Analysis for the Social Sciences_ in Example 7.9 on pages 355-362. The exercise provides an opportunity to estimate a binomial dependent variable and sets us up to talk about this example later when we talk about multi-level models. Also, a great opportunity to dive into Bayesian modeling in the context of a field experiment.\n",
    "\n",
    ">Prior to the presidential election in November 2004, we assembled a nationwide list of cable systems that covered only a single zip code. Small cable TV systems are a fertile source of experimental data for social scientists because their small size makes them inexpensive and conducive to large-N randomized studies. In order to test the televised messages in an environment that would not be dominated by other election-related advertisements, we removed all cable systems in 16 states that the Los Angeles Times classified as presidential battlegrounds (closely contested states). We then excluded any systems that had no time available in prime time during the week before the election or that cost more than 15 dollars per 30-second advertisement on the USA television network. We excluded all systems in Mississippi because its voter file is very difficult to obtain. This left 85 cable systems for randomization.\n",
    "\n",
    ">Random assignment of the cable systems took place as follows. Each system was matched with one or two other systems in the same state according to its past turnout rate in presidential elections. This procedure resulted in 40 strata containing the 85 cable systems. After sorting the list of 85 cable systems by strata and then by a random number, the first cable system in each stratum was assigned to the treatment condition, the others to control.\n",
    "\n",
    ">People living within the treatment systems saw two different 30-second advertisements produced by Rock the Vote. Both advertisements used the same format. The first dealt with the draft and the second, with education. In the draft advertisement, a young couple dancing at a party is talking about the man’s new job. He is very excited to be working in promotions and hopes to start his own firm in 6 months. The woman interrupts him and says, ‘‘That’s if you don’t get drafted.’’ The man is puzzled. She clarifies, ‘‘Drafted, for the war?’’ He responds, ‘‘Would they do that?’’ The advertisement closes with everyone at the party looking into the camera and the words, ‘‘It’s up to you’’ on the screen. The voiceover says, ‘‘The Draft. One of the issues that will be decided this November. Remember to vote on November 2nd.’’ The closing image is of the Rock the Vote logo on a black screen.\n",
    "\n",
    ">The second Rock the Vote advertisement dealt with education. A young man arrives at work with news that he has been accepted to college. His colleagues congratulate him and one of them asks, ‘‘Books, room, board, tuition ... how can you pay for all of that?’’ The advertisement closes with everyone looking out at the camera and the words, ‘‘It’s up to you’’ written on the screen. The voiceover is similar to the one above but with education substituted for draft. We showed both advertisements equally in all cable systems.\n",
    "\n",
    ">Each cable system comprises several thousand voters, and the entire data set encompasses approximately 850,000 registered voters. Of special  interest are the 23,869 voters who are 18 and 19 years of age, for whom this election represents the first federal election in which they are eligible to vote and to whom these ads were specifically addressed. The methodological question is what is the most efficient and reliable way to analyze these data? This question was particularly compelling since our previous mass-media turnout experiments suggested the effects of treatment were likely to be small in magnitude, but not zero (Vavreck  and Green 2006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file into a dictionary\n",
    "with open('data/rock_the_vote_data.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# close the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the source?\n",
    "print(json_data['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where can i get these data?\n",
    "print(json_data['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some info about the dataset\n",
    "print_info(json_data['info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what variables are in the dataset?\n",
    "print_vars(json_data['vars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just give it to me in a dataframe\n",
    "data = pd.DataFrame(json_data['data'])\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the question?\n",
    "\n",
    "We want to know if the _Rock the Vote_ TV advertisements had any affect on voter turnour among 18-19 year olds. Translating to statistics, is the distribution of the probability of voting in treated markets different from the distribution of the probability of voting in untreated markets? \"Distribution of probability\" corresponds to the distribution of the binomial parameter.\n",
    "\n",
    "Let's look at the distribution of the data to see if there is a difference between treated and untreated markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.kdeplot(data[data.treated == 1]['p'],shade=True,label='treated',color='darksalmon')\n",
    "sns.kdeplot(data[data.treated == 0]['p'],shade=True,label='untreated',color='cadetblue')\n",
    "\n",
    "# make it pretty\n",
    "sns.despine()\n",
    "\n",
    "# label the figure\n",
    "plt.title('Distribution of Turnout: Treated vs Untreated Markets',family='serif',size=14)\n",
    "plt.xlabel('theta',family='serif',size=12)\n",
    "plt.ylabel('density',family='serif',size=12)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is binomial data, we should look at the turnout rates as they relate to the number of registered voters in the market. Do we see more noise in smaller markets? Remember the Central Limit Theorem? If we see a small number of draws from a binomial distribution, the observed probability of success is likely to vary quite a bit. However, if we are lucky and get to see lots of data from a market, then the observed success rate is likely to be closer to the true, unobserved rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(data.n,data.p)\n",
    "\n",
    "# make it pretty\n",
    "sns.despine()\n",
    "\n",
    "# label the figure\n",
    "plt.title('Turnout versus Size of Market',family='serif',size=14)\n",
    "plt.xlabel('size of market',family='serif',size=12)\n",
    "plt.ylabel('turnout',family='serif',size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data from various binomials when the underlying rate of success is the same\n",
    "test_1 = sp.stats.binom.rvs(n=10,p=0.2,size=1000)/10\n",
    "test_2 = sp.stats.binom.rvs(n=100,p=0.2,size=1000)/100\n",
    "test_3 = sp.stats.binom.rvs(n=500,p=0.2,size=1000)/500\n",
    "\n",
    "# Initialise the figure and a subplot axes.\n",
    "num_rows = 1\n",
    "num_cols = 3\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(12, 4))\n",
    "\n",
    "# overall title\n",
    "fig.suptitle('Why do we see lots of variation in small markets?',y=1.05,fontsize=16,fontfamily='serif')\n",
    "\n",
    "# now plot the observed PDFs of the observed rate of success\n",
    "sns.kdeplot(test_1,shade=True,color='cornflowerblue',ax=ax[0])\n",
    "ax[0].set_title('Distribution of Success Rate 10 Trials',fontsize=12,fontfamily='serif')\n",
    "ax[0].set_xlim(0,1)\n",
    "\n",
    "sns.kdeplot(test_2,shade=True,color='cornflowerblue',ax=ax[1])\n",
    "ax[1].set_title('Distribution of Success Rate 100 Trials',fontsize=12,fontfamily='serif')\n",
    "ax[1].set_xlim(0,1)\n",
    "\n",
    "sns.kdeplot(test_3,shade=True,color='cornflowerblue',ax=ax[2])\n",
    "ax[2].set_title('Distribution of Success Rate 500 Trials',fontsize=12,fontfamily='serif')\n",
    "ax[2].set_xlim(0,1)\n",
    "\n",
    "# make the plot prettier\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling this thread a little further, suppose the true turnout rate in all 85 markets is the same and equals the observed mean, 0.53. How much variation would we expect to see from sampling? ie, ontological uncertainty introduced by random factors such as an 18 year old getting into a wreck on their way to the voting booth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean turnout rate across all 85 markets\n",
    "np.mean(data.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the figure and a subplot axes.\n",
    "num_rows = 3\n",
    "num_cols = 3\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
    "\n",
    "# overall title\n",
    "fig.suptitle('Simulations show variability across markets due to sampling',\n",
    "             y=1.05,fontsize=16,fontweight='bold',fontfamily='serif')\n",
    "\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # generate samples\n",
    "        sim_voters = sp.stats.binom.rvs(n=data.n,p=np.mean(data.p),size=len(data)) / data.n\n",
    "        \n",
    "        # plot simulated and actual\n",
    "        ax[i,j].scatter(data.n,sim_voters,s=12,color='cornflowerblue')\n",
    "        ax[i,j].scatter(data.n,data.p,s=6,alpha=0.75,color='salmon')\n",
    "        ax[i,j].axhline(0.55,alpha=0.25,lw=4,color='silver')\n",
    "        sns.despine(ax=ax[i,j])\n",
    "\n",
    "# make the plot prettier\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is more going on here than just sampling variation, but it does explain a good portion of overall variability in turnout rates across the 85 markets. So let's get to building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is our model?\n",
    "\n",
    "Let $i = 1, \\ldots N$ index the 85 markets in the data, and let $s = 1, \\ldots S$ index the 40 strata.\n",
    "\n",
    "We are modeling a random variable, $\\theta_i$, that is the probability of turnout in market $i$. This is a binomial likelihood because in each market there are a series of Bernoulli trials among registered voters who are 18 and 19 years old. \"Success\" occurs when one of these registered voters votes, \"failure\" occurs when they do not vote. \n",
    "\n",
    "Translating to the binomial distribution: \n",
    "* $n$ in the dataframe is the number of trials and is equal to the number of registered voters, \n",
    "* $r$ in the dataframe is the number of registered voters that voted\n",
    "* $p$ in the dataframe is the observed rate of success in market $i$.\n",
    "\n",
    "The complication here is that some markets are treated with _Rock the Vote_ adds. We let $T$ be a variable that equals 1 when the market was treated and 0 if it was not. In the dataframe this variable is ``treated``.\n",
    "\n",
    "Before we do that, we need to transform our random variable, $\\theta$. Why? $\\theta$ must live on $(0,1)$, but we want to decompose $\\theta$ into different parts. It is easier to do this if we don't have to worry about things adding up to 1. So we write $\\theta = f(\\alpha)$. $f(\\cdot)$ takes an $\\alpha$ which lies anywhere on the real line and maps it to a value between $(0,1)$.\n",
    "\n",
    "This transformation allows us to construct $\\alpha$ out of components that capture various parts of the system. Here are some ideas:\n",
    "1. Some markets are treated and some are not. We can specify a component of $alpha$ that captures this.\n",
    "2. Some markets are in different strata---which had similar turnout rates in past elections. How might this affect $\\theta$? \n",
    "3. Suppose we know what region each strata is in, we could account for similarities within a region.\n",
    "4. Suppose we knew past Republican vote totals for each market. Would this have an effect on $\\theta$?\n",
    "\n",
    "The only thing we know about a market is what strata it is in and whether it was treated. So to start let's assume that $\\alpha_i$ (our modeled turnout rate in market $i$) is a function of a common turnout rate plus the effect of the treatment:\n",
    "\n",
    "$\\alpha_i = \\alpha + \\delta * T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood\n",
    "\n",
    "$r_i \\sim \\textrm{Binomial}(N,\\theta_i)$\n",
    "\n",
    "where $\\theta_i = f(\\alpha_i)$\n",
    "and $\\alpha_i = \\alpha + \\delta * T$.\n",
    "\n",
    "What is $f(\\cdot)$? It is the inverse-logit function:\n",
    "\n",
    "$f(x) = \\textrm{logit}^-1(x) = \\frac{\\exp(x)}{1 + \\exp(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_logit(x):\n",
    "    return np.exp(x) / (1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,100)\n",
    "y = inv_logit(x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.xlabel(\"$f(x)$\")\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priors\n",
    "\n",
    "What are our parameters?\n",
    "\n",
    "* $\\alpha$ the common level of turnout\n",
    "* $\\delta$ the common treatment effect.\n",
    "\n",
    "Any ideas for what prior to put on these values? What do we know?\n",
    "\n",
    "* $\\alpha_i$ can be anything on the real line, so we do not want to restrict ourselves to distributions with positive support.\n",
    "* $\\alpha_i$ is continuous, so we need to focus on continuous probability distributions.\n",
    "\n",
    "Let's start with the normal distribution for each. Do we have any reason _a priori_ to specify a non-zero mean for either of these normal distributions? Unless we conjure up something, let's turn to the variances.\n",
    "\n",
    "To get a sense for what happens when we go from a normal distribution through the logit function and out to $\\theta \\in (0,1)$, let's simulate some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does that translate to theta?\n",
    "\n",
    "# Initialise the figure and a subplot axes.\n",
    "num_rows = 2\n",
    "num_cols = 2\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(10, 5))\n",
    "\n",
    "# overall title\n",
    "fig.suptitle('From normal to inverse logit',y=1.025,fontsize=14,fontweight='bold',fontfamily='serif')\n",
    "\n",
    "# sample from a normal(0,2)\n",
    "norm_samps = sp.stats.norm.rvs(loc=0,scale=2,size=1000)\n",
    "\n",
    "sns.kdeplot(norm_samps,shade=True,ax=ax[0,0])\n",
    "sns.despine(ax=ax[0,0])\n",
    "ax[0,0].set_title('x ~ normal(0,2)',fontfamily='serif')\n",
    "ax[0,0].set_xlim(-10,10)\n",
    "\n",
    "sns.kdeplot(logit(norm_samps),shade=True,ax=ax[0,1])\n",
    "sns.despine(ax=ax[0,1])\n",
    "ax[0,1].set_title('theta = invlogit(x)',fontfamily='serif')\n",
    "ax[0,1].set_xlim(0,1)\n",
    "\n",
    "# sample from a normal(0,1)\n",
    "norm_samps = sp.stats.norm.rvs(loc=0,scale=1,size=1000)\n",
    "\n",
    "sns.kdeplot(norm_samps,shade=True,ax=ax[1,0])\n",
    "sns.despine(ax=ax[1,0])\n",
    "ax[1,0].set_title('x ~ normal(0,1)',y=0.92,fontfamily='serif')\n",
    "ax[1,0].set_xlim(-10,10)\n",
    "\n",
    "sns.kdeplot(logit(norm_samps),shade=True,ax=ax[1,1])\n",
    "sns.despine(ax=ax[1,1])\n",
    "ax[1,1].set_title('theta = invlogit(x)',y=0.92,fontfamily='serif')\n",
    "ax[1,1].set_xlim(0,1)\n",
    "\n",
    "# make the plot prettier\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these simulations, do we want to have large variances on our normal distributions? The larger we make them, the more prior weight we put on extreme values of $\\theta$. Based on prior knowledge about overall election turnout, we can make a case for smaller variances.\n",
    "\n",
    "Checking this in Stan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's in this stan model anyway?\n",
    "f = open('rock_the_vote_prior.stan', 'r')\n",
    "file_contents = f.read()\n",
    "print (file_contents)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile Stan model\n",
    "pm = pystan.StanModel(file=\"rock_the_vote_prior.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_data = {'N':len(data),'reg_vtr':data.n,'votes':data.r,'T':data.treated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct MCMC using Stan\n",
    "pr_draws = pm.sampling(data=prior_data,iter=1000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give us a dictionary containing posterior draws for each parameter in the model\n",
    "pr_pd = pr_draws.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_pd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_pd['sim_votes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? np.random.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_idx = np.random.choice(range(len(data)),num_rows*num_cols,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the figure and a subplot axes.\n",
    "num_rows = 3\n",
    "num_cols = 3\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
    "\n",
    "# overall title\n",
    "fig.suptitle('Prior predictive checking: flip book',\n",
    "             y=1.05,fontsize=16,fontweight='bold',fontfamily='serif')\n",
    "\n",
    "rd_idx = np.random.choice(range(len(data)),num_rows*num_cols,replace=False)\n",
    "\n",
    "c = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # generate samples\n",
    "        sim_voters = pr_pd['sim_votes'][rd_idx[c],:] / data.n\n",
    "        \n",
    "        # plot simulated and actual\n",
    "        ax[i,j].scatter(data.n,sim_voters,s=12,color='cornflowerblue')\n",
    "        ax[i,j].scatter(data.n,data.p,s=6,alpha=0.75,color='salmon')\n",
    "        sns.despine(ax=ax[i,j])\n",
    "        c += 1\n",
    "\n",
    "# make the plot prettier\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile Stan model\n",
    "sm = pystan.StanModel(file=\"rock_the_vote.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data = {'N':len(data),'reg_vtr':data.n,'votes':data.r,'T':data.treated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct MCMC using Stan\n",
    "pst_draws = sm.sampling(data=stan_data,iter=1000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give us a dictionary containing posterior draws for each parameter in the model\n",
    "pst_pd = pst_draws.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_pd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pst_draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from look at the $\\theta$'s that the model finds that markets that were treated with _Rock the Vote_ advertisements had an 2 percentage point increase in turnout, from 0.53 to 0.55."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the figure and a subplot axes.\n",
    "num_rows = 4\n",
    "num_cols = 2\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(16, 16))\n",
    "\n",
    "# overall title\n",
    "fig.suptitle('Graphical depictions of the posterior',y=1.025,fontsize=18,fontfamily='serif')\n",
    "\n",
    "# ___ROW ONE___\n",
    "# **trace plots**\n",
    "ax[0,0].plot(pst_pd['alpha'],lw=1,alpha=0.75,color='cornflowerblue')\n",
    "ax[0,0].set_title('Trace plot of alpha',fontsize=12,fontfamily='serif')\n",
    "ax[0,1].plot(pst_pd['delta'],lw=1,alpha=0.75,color='cornflowerblue')\n",
    "ax[0,1].set_title('Trace plot of delta',fontsize=12,fontfamily='serif')\n",
    "\n",
    "# ___ROW TWO___\n",
    "# **joint distribution** of parameters\n",
    "sns.kdeplot(pr_pd['alpha'], pr_pd['delta'],\n",
    "            color='cornflowerblue', shade=True, shade_lowest=False,ax=ax[1,0])\n",
    "sns.kdeplot(pst_pd['alpha'], pst_pd['delta'],\n",
    "            color=\"salmon\", shade=True, shade_lowest=False,ax=ax[1,1])\n",
    "\n",
    "ax[1,0].set_title('Prior joint distribution of parameters',fontsize=12,fontfamily='serif')\n",
    "ax[1,0].set_xlim(-2,2)\n",
    "ax[1,0].set_ylim(-2,2)\n",
    "ax[1,1].set_title('Posterior joint distribution of parameters',fontsize=12,fontfamily='serif')\n",
    "ax[1,1].set_xlim(-2,2)\n",
    "ax[1,1].set_ylim(-2,2)\n",
    "\n",
    "# ___ROW THREE___\n",
    "# prior and posterior of **alpha**\n",
    "sns.kdeplot(pr_pd['alpha'],shade=True, lw=3,color='cornflowerblue',shade_lowest=False,ax=ax[2,0])\n",
    "sns.kdeplot(pst_pd['alpha'],shade=True, lw=3,color='salmon',shade_lowest=False,ax=ax[2,1])\n",
    "\n",
    "ax[2,0].set_title('Prior distribution of alpha',fontsize=12,fontfamily='serif')\n",
    "ax[2,0].set_xlim(-2,2)\n",
    "ax[2,1].set_title('Posterior distribution of alpha',fontsize=12,fontfamily='serif')\n",
    "ax[2,1].set_xlim(-2,2)\n",
    "\n",
    "# ___ROW FOUR___\n",
    "# prior and posterior of **delta**\n",
    "sns.kdeplot(pr_pd['delta'],shade=True, lw=3,color='cornflowerblue',shade_lowest=False,ax=ax[3,0])\n",
    "sns.kdeplot(pst_pd['delta'],shade=True, lw=3,color='salmon',shade_lowest=False,ax=ax[3,1])\n",
    "\n",
    "ax[3,0].set_title('Prior distribution of delta',fontsize=12,fontfamily='serif')\n",
    "ax[3,0].set_xlim(-2,2)\n",
    "ax[3,1].set_title('Posterior distribution of delta',fontsize=12,fontfamily='serif')\n",
    "ax[3,1].set_xlim(-2,2)\n",
    "\n",
    "\n",
    "# make the plot prettier\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot predicted against actual. We will sample from the posteriors for $\\theta_i$ then compare against the observed turnout rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise the figure and a subplot axes.\n",
    "num_rows = 3\n",
    "num_cols = 3\n",
    "fig, ax = plt.subplots(num_rows, num_cols,sharex=True, sharey=True, figsize=(12, 12))\n",
    "\n",
    "# overall title\n",
    "fig.suptitle('Posterior predictive checking: flip book',\n",
    "             y=1.05,fontsize=16,fontweight='bold',fontfamily='serif')\n",
    "\n",
    "c = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # generate samples\n",
    "        row_idx = np.random.randint(low=0,high=pst_pd['theta'].shape[0],size=pst_pd['theta'].shape[1])\n",
    "        col_idx = list(range(pst_pd['theta'].shape[1]))\n",
    "        idx = np.vstack([row_idx,col_idx]).T\n",
    "        rnd_post = [pst_pd['theta'][x[0],x[1]] for x in idx]\n",
    "        ppc_df = pd.DataFrame(np.vstack([rnd_post,data.p,data.treated]).T,\n",
    "                              columns=['sim_theta','obs_theta','treated'])\n",
    "\n",
    "        # plot data\n",
    "        sns.scatterplot(x=\"obs_theta\", y=\"sim_theta\", hue=\"treated\", data=ppc_df,ax=ax[i,j])\n",
    "  \n",
    "# make the plot prettier\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model comes no where near matching the observed data. We have constrained it with two parameters, but there are easy ways we can dramatically improve these fits! Stay tuned. \n",
    "\n",
    "In case you still want to know...did _Rock the Vote_ work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.treated[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are only two estimated rates of turnout, \n",
    "# we can just compare the posterior of one with and \n",
    "# without treatment\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.kdeplot(pst_pd['theta'][:,0],shade=True,label='untreated')\n",
    "sns.kdeplot(pst_pd['theta'][:,1],shade=True,label='treated')\n",
    "plt.xlabel('voter turnout');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
