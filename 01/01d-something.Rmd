---
title: 'Week 1 lab: Something'
output: 
    github_document:
        pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

```{r load-libraries}
library(tidyverse)

theme_set(theme_minimal())
```

## Notes on Lambert


### Central Limit Theorem
CLT says that the sample means of a distribution will be normally distributed around the population mean. Why is that? Well if you think about it, if you take two random samples from a distribution and then average them there are way more ways to get something close to the mean than there are to get something at the extremes. For example, in a uniform distribution of numbers between 0 and 1, there are many ways to get an average close to 0.5 and very few ways to get an average close to 0 or 1. It turns out that if you count up the different ways it will start to resemble a Normal distribution.

### Equivalence Principle
The probability of data conditioned on a parameter equals the likelihood of that parameter value given the same data.

$$P(X|\theta) = L(\theta|X)$$

For example, consider a coin flip where the probability of getting heads is $\theta$. Then we know that the probability of $P(X = \text{heads}|\theta) = \theta$). Alternatively, if we get a heads and want to know the likely values of $\theta$, we can write $L(\theta|X = \text{heads}) = \theta$. The likelihood is not a proper probability distribution because it does not sum to one over $\theta$.

Take an example where we see a sequence of two heads and one tail. The probability of this sequence is $P(X = \text(HHT)|\theta) = \theta^2(1 - \theta)$. The likelihood is the same $L(\theta|\text{HHT}) = \theta^2(1 - \theta)$

```{r equivalence-principle}
tibble(theta = seq(0, 1, length = 100)) %>% 
    ggplot(aes(x = theta, y = theta^2*(1-theta))) + 
    geom_line() +
    labs(
        subtitle = "Theta more likely to be a high value after seeing a HHT sequence",
        title = "Likelihood of theta given a sequence of HHT"
    )
```

### Maximum Likelihood: Beer Example
Imagine we are working in a bar and we observe the waiting times between consequetive beer orders. How should we choose a suitable likelihood for waiting times? We will assume:

- each beer order is independent. This is probably not true. People usually goto bars in groups!
- the average rate of beers/minute is constant. Probably also not true. People probably order less at lunch time than at dinner on average.

Given these constraints, we choose the Exponential distribution. This says that the waiting time between beer orders looks like $T_i \sim \text{exp}(\lambda) = \lambda\text{exp}^{-\lambda T_i}$.

This looks like this for varying values of $\lambda$
```{r exponential-distribution}
tibble(
    lambda = rep(c(0.25, 0.5, 1, 2), each = 1000),
    t = rep(seq(0, 8, length = 1000), 4), 
    y = lambda*exp(-lambda*t)
) %>% 
    ggplot(aes(x = t, y = y, color = factor(lambda))) +
    geom_line() +
    scale_color_discrete("lambda") +
    labs(title = "Exponential distribution for various lambdas")
```

Let's say we observe the following waiting times: $1, 0.5, 3, 2$. What does the likelihood function look like for $\lambda$?

```{r exponential-likelihood}
x <- c(1, 0.5, 3, 2)
n <- length(x)

tibble(
    lambda = seq(0, 8, length = 1000),
    likelihood = lambda^n*exp(-lambda*sum(x))
) %>% 
    ggplot(aes(x = lambda, y = likelihood)) +
    geom_line()
```

Let's turn this into a function so we can visualize the likelihood for arbitrary data.
```{r plot-likelihood}
plot_likelihood <- function(x) {
    n <- length(x)
    data <- tibble(
        lambda = seq(0, 10, length = 1000),
        likelihood = lambda^n*exp(-lambda*sum(x))
    )

    # plot
    ggplot(data, aes(x = lambda, y = likelihood)) + geom_line()
}

plot_likelihood(c(0.1, 0.2, 0.3))
```

### Grid Approximation
